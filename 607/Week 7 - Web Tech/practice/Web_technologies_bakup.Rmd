---
title: "Web Technologies"
author: "Shyam BV"
date: "October 12, 2016"
output: html_document
---

## Packages used

RCurl
XML
rjson
selectr
ROAuth
httr
rvest
stringr

RCurl R interface to the libcurl library for making general HTTP requests
RHTMLForms Tools to process Web/HTML forms
XML Tools for parsing XML and HTML documents and working with structured data from the Web
RJSONIO Functions for handling JSON data
jsonlite Functions for handling JSON data
rjson Functions for handling JSON data
ROAuth Interface for authentication via OAuth 1.0
SSOAP Use SOAP protocol to retrieve data



```{r loadlibrary,eval=TRUE}
install.packages("gdata")
install.packages("RCurl")
install.packages("XML")
install.packages("httr")
library("httr")


library(XML)

library(gdata)
library(RCurl)
```



```{r webdata,eval=TRUE}

r_home = url("http://www.r-project.org/")

moby_url <- url("http://www.gutenberg.org/ebooks/2701.txt.utf-8")

moby_ick <- readLines(moby_url,n=500)


str(moby_ick)

moby_ick[1:5]

mobydick.txt <- download.file("http://www.gutenberg.org/cache/epub/2701/pg2701.txt","mobydick.txt")


moby_dick_chap1 = rep("", 10)
# number of lines to skip until Chapter 1
skip = 535
# reading 10 lines (line-by-line using scan)
for (i in 1L:10) {
one_line = scan("mobydick.txt", what = "", skip = skip, nlines = 1)
# pasting the contents in one_line
moby_dick_chap1[i] = paste(one_line, collapse = " ")
skip = skip + 1
}


```



```{r webdata1,eval=TRUE}
#Simple read 

skulls = readLines("http://lib.stat.cmu.edu/DASL/Datafiles/EgyptianSkulls.html",100)
readLines("https://stainwash.in/pricing",n=100)


taxon_url = "http://www.bio.ic.ac.uk/research/mjcraw/therbook/data/taxon.txt"
# import data in R
taxon = read.table(taxon_url, header = TRUE, row.names = 1)

#Excel sheet
alpha_xls = "http://www.lsi.upc.edu/Ëœbelanche/Docencia/mineria/Practiques/alpha.xls"

sheetCount(alpha_xls)

#HTTPS

iris_file <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data",header  =FALSE)

getURL("https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data")


url("https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data")

iris_file <- "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"

iris_url = getURL(iris_file)
iris_data = read.csv(textConnection(iris_url), header = FALSE)


#HTTPS google sheet

google_docs <- "https://docs.google.com/spreadsheets/d/"

email_key <- "https://docs.google.com/spreadsheets/d/1Xarr8T0JTIsZvW84zPhiyhDCGitwHFvxKYHf_jCd2yU"

email_add <- getURL(email_key)

email <- read.csv(textConnection(email_add),header=TRUE)


swim_wiki = "https://en.wikipedia.org/wiki/World_record_progression_1500_metres_freestyle"

swim_url <- getURL(swim_wiki)

swim1500 = readHTMLTable(textConnection(swim_url), which = 1, stringsAsFactors = FALSE)



#health URL


cafile <- system.file("CurlSSL", "cacert.pem", package = "RCurl")

# Read page
page <- GET(
  "https://ned.nih.gov/", 
  path="search/ViewDetails.aspx", 
  query="NIHID=0010121048",
  config(cainfo = cafile)
)

# Use regex to extract the desired table
x <- content(page,as="text")
tab <- sub('.*(<table class="grid".*?>.*</table>).*', '\\1', x)

# Parse the table
readHTMLTable(tab)
paste(health_read, collapse = " ")

remove(cafile)




health_url <- url("https://ned.nih.gov/search/ViewDetails.aspx?NIHID=0010121048")
health_read <- readLines(health_url)


health_read <- url("https://ned.nih.gov/search/ViewDetails.aspx?NIHID=0010121048")

tab1 <- sub('.*(<table class="grid".*?>.*</table>).*', '\\1', paste(health_read, collapse = " "))
readHTMLTable(tab1)




wiki_url <- url("https://en.wikipedia.org/wiki/World_record_progression_1500_metres_freestyle")

wiki_read <- readLines(wiki_url)


wiki_tab1 <- sub('.*(<table class="wikitable".*?>.*</table>).*', '\\1', paste(wiki_read, collapse = " "))

test <- readHTMLTable(wiki_tab1)

test[[1]]

```


```{r xmlparsing, eval=TRUE}

doc1 = xmlParse("http://www.xmlfiles.com/examples/plant_catalog.xml")

str(doc1)

doc2 = xmlParse("http://www.xmlfiles.com/examples/plant_catalog.xml",useInternalNodes = FALSE)

doc3 = xmlTreeParse("http://www.xmlfiles.com/examples/plant_catalog.xml")

doc5 = htmlParse("http://www.r-project.org/mail.html")

xml_string = c(
'<?xml version="1.0" encoding="UTF-8"?>',
'<movies>',
'<movie mins="126" lang="eng">',
'<title>Good Will Hunting</title>',
'<director>',
'<first_name>Gus</first_name>',
'<last_name>Van Sant</last_name>',
'</director>',
'<year>1998</year>',
'<genre>drama</genre>',
'</movie>',
'<movie mins="106" lang="spa">',
'<title>Y tu mama tambien</title>',
'<director>',
'<first_name>Alfonso</first_name>',
'<last_name>Cuaron</last_name>',
'</director>',
'<year>2001</year>',
'<genre>drama</genre>',
'</movie>',
'</movies>')


movies_xml = xmlParse(xml_string, asText = TRUE)

class(movies_xml)

root <- xmlRoot(movies_xml)

class(root)

movie_child <- xmlChildren(root)

xmlValue(root) 

leafnode <- xmlChildren(root)[[2]][[2]][[2]]


leafnodemovie <- xmlChildren(root)[[2]][[2]]

xmlParent(leafnode)

getSibling(leafnodemovie)


##looping xml

sapply(movie_child, names)

```



